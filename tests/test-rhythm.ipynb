{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Test rhythm tracking approaches\n",
    "Tests were done on 31 songs, information about them can be found in file `data.json`.\n",
    "For testing, methods from `detect.py` were used. \n",
    "\n",
    "Each generated file with rhythm track was then played to check if the generated clicks align with strong note onsets and if they follow some patterns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c1afa1b3c91ea00"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile as wav\n",
    "import librosa\n",
    "import RhythmRecognition.detect\n",
    "from RhythmRecognition.constants import *\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "f = open('data.json')\n",
    "songs = json.load(f)\n",
    "f.close()\n",
    "path = \"../audio_files/\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T21:40:24.338792400Z",
     "start_time": "2024-05-06T21:40:15.277857500Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def save(signal, rhythm_track, filename):\n",
    "    rhythm_track = librosa.clicks(times=rhythm_track, sr=SAMPLING_RATE, length=len(signal), hop_length=HOP_LENGTH)\n",
    "    combined = signal + rhythm_track\n",
    "    wav.write(filename, SAMPLING_RATE, combined.astype(signal.dtype))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T21:40:24.338792400Z",
     "start_time": "2024-05-06T21:40:24.321009300Z"
    }
   },
   "id": "68448e3487a0b93e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rhythm track will be generated using known tempo (it will not be calculated using any of the tempogram methods) so we can be sure that the used tempo is correct.\n",
    "\n",
    "Also, in beat detection, the score-based approach will be used as it worked better than penalty-based approach. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "424ea365cab622c3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/31, 1/31, 2/31, 3/31, 4/31, 5/31, 6/31, 7/31, 8/31, 9/31, 10/31, 11/31, 12/31, 13/31, 14/31, 15/31, 16/31, 17/31, 18/31, 19/31, 20/31, 21/31, 22/31, 23/31, 24/31, 25/31, 26/31, 27/31, 28/31, 29/31, 30/31, "
     ]
    }
   ],
   "source": [
    "duration = 40\n",
    "\n",
    "for i in range(len(songs)):\n",
    "    print(i, \"/\", len(songs), sep=\"\", end=\", \")\n",
    "    song = songs[i]\n",
    "    audiofile = path + song[\"filename\"]\n",
    "    \n",
    "    signal, _ = librosa.load(audiofile, duration=duration, sr=SAMPLING_RATE)  # load the audio file\n",
    "    \n",
    "    # generate and save rhythm track for different approaches\n",
    "    \n",
    "    rhythm_track = RhythmRecognition.detect.rhythm_track(audiofile, approach=\"parts\", bpm=song[\"tempo\"], \n",
    "                                                         beat_approach=\"score\",novelty_approach=\"spectral\")\n",
    "    save(signal, rhythm_track, \"out/rhythm/parts/spectral/\" + song[\"name\"] + str(i) + \".wav\")\n",
    "    \n",
    "    rhythm_track = RhythmRecognition.detect.rhythm_track(audiofile, approach=\"parts\", bpm=song[\"tempo\"], \n",
    "                                                         beat_approach=\"score\",novelty_approach=\"energy\")\n",
    "    save(signal, rhythm_track, \"out/rhythm/parts/energy/\" + song[\"name\"] + str(i) + \".wav\")\n",
    "    \n",
    "    rhythm_track = RhythmRecognition.detect.rhythm_track(audiofile, approach=\"chorus-verse\", bpm=song[\"tempo\"], \n",
    "                                                         beat_approach=\"score\",novelty_approach=\"spectral\")\n",
    "    save(signal, rhythm_track, \"out/rhythm/chorus-verse/spectral/\" + song[\"name\"] + str(i) + \".wav\")\n",
    "    \n",
    "    rhythm_track = RhythmRecognition.detect.rhythm_track(audiofile, approach=\"chorus-verse\", bpm=song[\"tempo\"], \n",
    "                                                         beat_approach=\"score\",novelty_approach=\"energy\")\n",
    "    save(signal, rhythm_track, \"out/rhythm/chorus-verse/energy/\" + song[\"name\"] + str(i) + \".wav\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T21:50:21.392599300Z",
     "start_time": "2024-05-06T21:40:24.325766400Z"
    }
   },
   "id": "4fd90bef68b14761"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T21:50:21.396567400Z",
     "start_time": "2024-05-06T21:50:21.394538800Z"
    }
   },
   "id": "551149223ecddad4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
